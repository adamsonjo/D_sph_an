{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8680eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Пользователь\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "\n",
    "# Загружаем необходимые ресурсы\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Функция для чтения данных из файла\n",
    "def read_sid_phrases(file_path):\n",
    "    \"\"\"Чтение фраз из одного файла.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            sid_phrases = file.readlines()\n",
    "        return [phrase.strip() for phrase in sid_phrases]  # Убираем лишние пробелы и символы новой строки\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при чтении файла {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Функция для чтения всех файлов из директории\n",
    "def read_multiple_files(directory_path):\n",
    "    \"\"\"Чтение всех файлов в указанной директории.\"\"\"\n",
    "    sid_phrases = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):  # Можно изменить на другой формат, если нужно\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            sid_phrases.extend(read_sid_phrases(file_path))\n",
    "    return sid_phrases\n",
    "\n",
    "# Пример пути к директории с файлами\n",
    "directory_path = 'C:\\\\Users\\\\Пользователь\\\\Desktop\\\\Python Projects\\\\Data Analyze\\\\seed_phrases'  # Замените на путь к вашей директории с файлами\n",
    "\n",
    "# Чтение данных из всех файлов в директории\n",
    "sid_phrases = read_multiple_files(directory_path)\n",
    "\n",
    "# Проверка, если фразы не загружены\n",
    "if not sid_phrases:\n",
    "    print(\"Не удалось загрузить данные. Проверьте путь к файлам.\")\n",
    "    exit()\n",
    "\n",
    "# Предобработка текста: токенизация, удаление стоп-слов и пунктуации\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Токенизация текста, удаление стоп-слов и символов.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))  # Стоп-слова для английского языка\n",
    "    tokens = word_tokenize(text.lower())  # Преобразуем текст в токены (слова)\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]  # Оставляем только слова (без цифр и пунктуации)\n",
    "    return tokens\n",
    "\n",
    "# Обрабатываем все строки (каждая строка — это отдельная фраза или последовательность слов)\n",
    "all_tokens = []\n",
    "for phrase in sid_phrases:\n",
    "    all_tokens.extend(preprocess_text(phrase))\n",
    "\n",
    "# Частотный анализ слов\n",
    "word_counts = Counter(all_tokens)\n",
    "\n",
    "# Создание списка всех уникальных слов\n",
    "vocab = list(word_counts.keys())\n",
    "\n",
    "# Создание словаря для отображения индексов слов\n",
    "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "\n",
    "# Инициализация совместной матрицы частоты\n",
    "window_size = 5  # Размер окна для подсчета совместной частоты слов\n",
    "co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "# Подсчет совместных частот слов в окне\n",
    "for i in range(len(all_tokens) - window_size + 1):\n",
    "    window = all_tokens[i:i + window_size]\n",
    "    for j in range(len(window)):\n",
    "        for k in range(j + 1, len(window)):\n",
    "            word_j = window[j]\n",
    "            word_k = window[k]\n",
    "            if word_j in word_to_index and word_k in word_to_index:\n",
    "                idx_j = word_to_index[word_j]\n",
    "                idx_k = word_to_index[word_k]\n",
    "                co_occurrence_matrix[idx_j][idx_k] += 1\n",
    "                co_occurrence_matrix[idx_k][idx_j] += 1  # Симметричная матрица\n",
    "\n",
    "# Нормализуем матрицу, чтобы значения были в диапазоне от 0 до 1\n",
    "co_occurrence_matrix = co_occurrence_matrix / np.max(co_occurrence_matrix)\n",
    "\n",
    "# Преобразование в DataFrame для удобного отображения\n",
    "co_occurrence_df = pd.DataFrame(co_occurrence_matrix, index=vocab, columns=vocab)\n",
    "\n",
    "# Визуализация - Тепловая карта совместной частоты\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(co_occurrence_df, annot=False, cmap='YlGnBu', xticklabels=10, yticklabels=10)\n",
    "plt.title(\"Тепловая карта совместной частоты слов\")\n",
    "plt.xlabel(\"Слова\")\n",
    "plt.ylabel(\"Слова\")\n",
    "plt.savefig(\"co_occurrence_heatmap.png\")  # Сохранение изображения\n",
    "plt.close()  # Закрытие текущей фигуры\n",
    "\n",
    "# Рассчитываем косинусное сходство между словами\n",
    "cosine_sim = cosine_similarity(co_occurrence_matrix)\n",
    "\n",
    "# Преобразуем матрицу сходства в DataFrame\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim, index=vocab, columns=vocab)\n",
    "\n",
    "# Визуализация - Тепловая карта косинусного сходства\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cosine_sim_df, annot=False, cmap='YlGnBu', xticklabels=10, yticklabels=10)\n",
    "plt.title(\"Тепловая карта косинусного сходства слов\")\n",
    "plt.xlabel(\"Слова\")\n",
    "plt.ylabel(\"Слова\")\n",
    "plt.savefig(\"cosine_similarity_heatmap.png\")  # Сохранение изображения\n",
    "plt.close()  # Закрытие текущей фигуры\n",
    "\n",
    "# Визуализация - Облако слов для самых частых слов\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_counts)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Облако слов для сид-фраз\")\n",
    "plt.savefig(\"wordcloud.png\")  # Сохранение изображения\n",
    "plt.close()  # Закрытие текущей фигуры"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
